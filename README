This repositiory contains both python and matlab files to load training and testing data for the brain decoding competition
All of the python scripts require the scipy stack. I just use the Anaconda python distribution, which can be found here
http://continuum.io/downloads

Python scripts: 
make_features.py - makes features from the mat files. The training data will be stored
in a matrix called X_train. Each row of X_train is an observation and each column is a feature.
The labels for the observations are stored in the vector called y_train. Before you can run 
this script you need to have downloaded all of the data from the Kaggle website.

Matlab scripts:
make_features.m - Does the same thing as the python version of make_features. 
create_features.m - A function used by make_features 

My Approach so Far: 
    So far I compress the features using Kernel Principal Component Analysis (KernelPCA). A description of the
KernelPCA can be found here http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html
This both reduces the dimensionality of the data and maps it to a higher dimensional space. A sigmoid Kernel is used. 
    I then use Logisitic Regression with Stochastic Gradient Descent. This is just a generic linear classifier
which is really fast to train. A description Stochastic Gradient Descent and Logistic Regression can be found here
http://scikit-learn.org/stable/modules/sgd.html#classification    


Ideas to Increase Performance: 
    Take a look at this paper http://arxiv.org/pdf/1404.4175v1.pdf. It talks about transfer learning. I think I really need to incorporate the 
density ratio mentioned in this paper to really get an increase in the accuracy. This allows the samples in the training set to be weighted
based on how similar they are to samples in the test set. 
    I also need to extract some useful features. As of right now I am only using the time series data. 
    
    
Let me know if you have any questions or ideas and Thanks!!!!
